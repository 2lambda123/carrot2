TODO:

* Investigate a query in Polish: miś

* (Other Topics) as the only subcluster

* words that may not be stopwords, but are not allowed to form cluster labels alone

* mgr, dr, hab, prof in Polish results

* higher score when a word occurs in the title

* skip content in parentheses?

* first length-normalisation and then idf scaling?

* switch to df cutoff in term selection?

* case normalisation: LaTeX Tutorial vs. LaTeX Rubber

* problems with snippet readers?

* repeated labels across passes (Digital Library Framevork vs. _dLibra_
  Digital Library Framework)

* is a hyphen a sentence separator?

* Gawronka PLFOTO group - assignment bug?

* eliminate groups with identical content?

* cluster label longer than 2 characters?

* choose which word cannot be omitted (use tf-df)

* incorporate the search engine's ranking to the cluster ranking

* national characters -> english spelling: Stanis�aw -> Stanislaw

* Have look at JADE, which may offer a nice performance boost

* FeatureSelectionStrategy interface is meant to return single words found in 
  the input. In case more complex features are needed, a similar interface, 
  called e.g. FeatureExtractionStrategy can be created.
  
* Multilingual implementations of the FeatureSelectionStrategy,
  TdMatrixBuildingStrategy and PhraseExtractionStrategy interfaces are possible, 
  but delayed for the time being:
  
  	- FeatureSelectionStrategy: in the frequency information include only 
  	  those occurrences that are not stop-words in the context of their enclosing 
  	  document (e.g. treat the word 'ale' as a stop word in Polish documents, 
  	  but not in English ones). Thus, it is possible that a word that is a 
  	  stop word in one language will be included on the list because it is a 
  	  frequent non-stop-word in one or more other languages.
  	  
  	- TdMatrixBuildingStrategy: for each document mark occurrences of terms that
  	  are non-stop-words in the document's language. E.g. for a Polish document
  	  the word 'ale' will have a zero weight, but for an English document 
  	  containing that word the corresponding weight will not be zero.
  	  
  	- PhraseExtractionStrategy: discover phrases separately for each language to
  	  avoid sequences beginning/ending in stopwords. Merge the lists afterwards
  	  to aggregate common 'international' phrases.

  	- Wouldn't it be enough to have separate codes for the same word? Not really.
  	  Ideally, Polish 'ale' would be different the English one, but 'Microsoft' 
  	  should have the same code no matter the context document's language
  	  
* Improved language guessing
	- for extremely short strings it seems that the stop word method would 
	  perfom better. In case of draws - let the trigram method decide.
	  
	- it would be nice to have the documents _first_ tokenised and only then
	  language-recognised. This is because the n-grams would be calculated only
	  for non-noise content (or the tokenisation would be performed only once).
	  This would involve splitting the tokenisation and stemming/stopword marking
	  into two separate filters.

* Tokenizer improvements
    - improve tokenizer:
      - HTML entities: remove (e.g. &quot;), convert to characters (e.g. &amp;)
      - treat (letter|digit)*(.(letter|digit)*)* as an URL. To be a sentence 
        separator the period must be followed by a white space
    - nasty heuristic: a cluster label must be at least x (e.g. 3) characters long
      and can't consist of numbers only
      
DONE:

* Stemming of Phrases: 
	- define classes of equivalence among term codes based on stemming information
	- return a group of equivalent phrases with one aggregated frequency

* When light-speed implementation is needed, TDMatrixBuildingStrategy, 
  FeatureSelectionStrategy and PhraseExtractionStrategy can be implemented in 
  one class, reusing common information created during each of the processes.
  
* tokenizer: "Of- Speech"

* "on-line" as a stop-word?

* month names -> stopwords?
  
* (Ronnie) O'Sullivan (apostrophe treatment)

