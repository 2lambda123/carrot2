<article>
  <h1>Tuning performance</h1>

  <p>
    This document presents some hints to improve clustering performance, especially for larger data
    sets.
  </p>

  <p>
    Carrot<sup>2</sup> clustering algorithms have been designed to work really fast
    but the tradeoff is that they store all the data structures in memory (and require the documents
    to be passed via memory). The size of the virtual machine's heap will increase quickly
    with longer overall size of input text. Also, the more documents you put on input and the longer
    the documents are, the longer the clustering time will be.
  </p>

  <p>
    Below are a few generic guidelines on improving performance should it become a problem.
  </p>

  <section id="reduce-input-size">
    <h2>Reduce size of input</h2>

    <p>
      In many cases clustering short document excerpts may work just as well or
      even better than full documents. Consider the possibility of replacing
      full content with:
    </p>

    <ul>
      <li>query-matching document fragments (such as search result snippets), if input documents
        are a result of some type of user-entered query
      </li>
      <li>titles and abstracts of documents, if they are available,</li>
      <li>just the leading few sentences or paragraphs of each document.</li>
    </ul>
  </section>

  <section id="batch-and-merge">
    <h2>Batch and merge smaller clustering runs</h2>

    <p>
      In many cases when the input collection of documents is too large to cluster as a whole,
      dividing the input into smaller batches (or sampling smaller batches from the input), then
      clustering separately and finally merging
      based on cluster label text gives very reasonable results.
    </p>

    <p>
      The above approach works because cluster labels recurring in smaller batches are very likely
      to be significant for the entire collection. The downside is that
      very small clusters containing just a few documents are likely to be lost during this process.
    </p>
  </section>

  <section id="tune-algorithm">
    <h2>Tune algorithm attributes</h2>

    <p>
      In many cases the default attribute settings for each algorithm may not be suitable for very
      large inputs. Here are some attributes tweak suggestions you should consider
      (experimentally adjusting their value to the size of your particular input).
    </p>

    <h3>STC, Bisecting k-Means</h3>

    <p><strong><code>wordDfThreshold</code></strong></p>
    <p>
      Increase the minimum document frequency (minimum number of occurrences) of terms and phrases
      to a higher value. Something like 0.5% of the number of documents will typically work (so for
      a document collection of 5000 documents set the attribute to 25).
    </p>


    <h3>Lingo</h3>

    <p><strong><code>wordDfThreshold</code>, <code>phraseDfThreshold</code></strong></p>
    <p>
      Increase the minimum document frequency (minimum number of occurrences) of terms and phrases
      to a higher value. Something like 0.5% of the number of documents will typically work (so for
      a document collection of 5000 documents set the attribute to 25).
    </p>

    <p><strong><code>factorizationQuality</code></strong></p>
    <p>
      For <code>algorithm.matrixReducer.factorizationFactory</code> implementations that support
      this attribute
      (and nearly all of them do), lower <code>factorizationQuality</code>. This will cause the
      matrix factorization algorithm to perform fewer iterations and hence complete quicker.
    </p>
    <p>
      Alternatively, you can set <code>algorithm.matrixReducer.factorizationFactory</code>
      to an implementation of <code>PartialSingularValueDecompositionFactory</code>, which is
      slightly faster than the other factorizations and does not have
      any explicit <code>factorizationQuality</code> attribute.
    </p>

    <p><strong><code>maximumMatrixSize</code></strong></p>
    <p>
      Lower maximum matrix size in <code>matrixBuilder</code>. This will cause the matrix
      factorization algorithm to complete quicker and use less memory. The tradeoff is that with
      small matrix sizes, Lingo may not be able to discover smaller clusters.
    </p>
  </section>

</article>
